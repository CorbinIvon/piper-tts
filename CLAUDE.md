# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a Piper TTS voice training framework for creating custom text-to-speech voices. The project consists of two main workflows:

1. **TTS Inference**: Using pre-trained or custom-trained voice models to generate speech
2. **Voice Training**: Processing audio recordings and training custom voice models

## Environment Setup

Always activate the virtual environment before running commands:
```bash
. .venv/bin/activate
```

Dependencies are managed in `requirements.txt` and include PyTorch, librosa, piper-tts, and audio processing libraries.

## Key Commands

### Running TTS Synthesis
```bash
# Basic usage
python hello_world_tts.py "Text to speak" output.wav path/to/model.onnx

# The script requires a voice model (.onnx file)
# Pre-trained models: https://github.com/rhasspy/piper/releases/tag/v1.0.0
```

### Data Preparation Pipeline
```bash
# Process audio directory into training dataset
python voice_training/data_preparation.py <audio_directory> [speaker_id]

# Example:
python voice_training/data_preparation.py ./my_recordings my_voice

# Output: training_data/manifest.jsonl and processed audio in training_data/audio/
```

### Model Training
```bash
# Train a voice model
python voice_training/trainer.py <manifest_path> [output_dir]

# Example:
python voice_training/trainer.py training_data/manifest.jsonl models/my_voice

# Monitor training with TensorBoard:
tensorboard --logdir models/my_voice/logs
```

## Architecture

### Data Flow Pipeline

```
Raw Audio → VoiceDataPreparator → Training Dataset → VoiceTrainer → ONNX Model → Piper TTS
```

### Module Organization

**`hello_world_tts.py`**: Simple TTS inference script
- Uses `PiperVoice.load()` to load ONNX models
- Writes audio output using Python's `wave` module
- Command-line interface for quick testing

**`voice_training/data_preparation.py`**: Audio preprocessing pipeline
- `VoiceDataPreparator` class handles all preprocessing
- Normalizes audio to 22050 Hz sample rate
- Automatically splits long audio (>10s) into chunks
- Trims silence using librosa (top_db=20)
- Outputs manifest.jsonl with metadata for each audio sample

**`voice_training/trainer.py`**: Training framework
- `VoiceDataset`: PyTorch Dataset that loads from manifest.jsonl
- Converts audio to mel spectrograms (n_fft=1024, hop_length=256, n_mels=80)
- `VoiceTrainer`: Training orchestration with TensorBoard logging
- **Important**: The trainer is a framework/skeleton - actual TTS model architecture needs to be implemented

### Audio Processing Details

The data preparation pipeline performs:
1. Load audio at 22050 Hz (mono)
2. Normalize amplitude using `librosa.util.normalize()`
3. Trim silence from start/end
4. Split into ≤10 second chunks (minimum 1 second)
5. Save as individual WAV files with naming: `{speaker_id}_{original_stem}_{chunk_idx:03d}.wav`

The manifest.jsonl format:
```json
{"audio_path": "training_data/audio/...", "speaker_id": "...", "duration": 5.2, "text": "...", "sample_rate": 22050}
```

### Training Pipeline Architecture

1. **VoiceDataset** loads manifest and converts to mel spectrograms on-the-fly
2. **Custom collate function** handles variable-length audio by padding to batch max length
3. **Trainer framework** includes:
   - GPU/CPU device selection
   - TensorBoard integration
   - Checkpoint saving (every 10 epochs)
   - Placeholder for model architecture (needs implementation)

**Critical Note**: The `trainer.py` contains TODO comments indicating where to implement:
- TTS model architecture (Tacotron, FastSpeech, VITS, etc.)
- Forward pass and loss calculation
- ONNX export logic

## Important Constraints

### Training Model Architecture
The current `trainer.py` is a framework only. To make it functional:
1. Choose a TTS architecture (referenced in VOICE_TRAINING_GUIDE.md)
2. Implement model initialization in the `train()` method
3. Add loss functions and optimization loop
4. Implement ONNX export in `export_onnx()` method

Alternatively, fine-tune an existing Piper model rather than training from scratch.

### Audio Data Requirements
- Minimum: 30 minutes of audio (will produce recognizable but robotic voice)
- Recommended: 1+ hours of audio for good quality
- Must be consistent recording environment and microphone
- Supported formats: .wav, .mp3, .flac, .m4a, .ogg

### Hardware Considerations
- **Training**: GPU highly recommended (6GB+ VRAM). CPU training is extremely slow.
- **Inference**: CPU is sufficient; Piper is optimized for CPU inference.

## Project Structure Patterns

Files are organized by workflow stage:
- Root level: Inference scripts (`hello_world_tts.py`)
- `voice_training/`: All training-related modules
- `training_data/`: Generated by data preparation (not in repo)
- `models/`: Generated by training (not in repo)

The `voice_training/__init__.py` exports main classes for use as a package.

## Working with Audio

When processing audio files:
- Sample rate is standardized to 22050 Hz throughout the pipeline
- Mel spectrogram parameters are fixed (ensure consistency if modifying)
- Duration threshold for splitting is 10 seconds (configurable in `VoiceDataPreparator.target_duration`)
- Silence trimming uses 20 dB threshold (configurable in `process_audio_file()`)

## Testing Workflow

To test the full pipeline:
1. Download a pre-trained voice model from Piper releases
2. Run `hello_world_tts.py` to verify inference works
3. Prepare a small test dataset with `data_preparation.py`
4. Verify manifest.jsonl is created correctly
5. Run trainer (though it won't train without model implementation)

## Model Files

Voice models consist of:
- `.onnx` file: The neural network model
- `.onnx.json` file: Model configuration (required by Piper, mentioned in docs)

When implementing custom training, ensure both files are generated for Piper compatibility.
